{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a61bea4-1d0b-4b97-9c29-c4db350d4542",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Sarale Goldberger\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a1fd1f-c28b-4e21-a4eb-fa98ab1d015c",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "1. Make sure you have filled out your \"NAME\" and \"COLLABORATORS\" (if any) in the previous cell.\n",
    "\n",
    "2. You should complete all code/markdown cells that state \"YOUR CODE HERE\" or \"YOUR ANSWER HERE\". \n",
    "   \n",
    "3. Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "4. Partial credit can be obtained if your solution approach is clear and the documented within comments in the implementation.\n",
    "\n",
    "5. You should follow good coding practices. Your code should use type hints, be robust against invalid inputs, and you should also write a few test cases to check for correctness particularly including edge cases.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e64a0-6a08-4c69-84d3-22c5173488d4",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "In this homework, we will build a few models from scratch and then use them to explore a real world dataset.  We are hoping to get some insight into the following topics:\n",
    "\n",
    "1. How does the training process work?\n",
    "2. What is the flow for the prediction process?\n",
    "3. What does a model even look like?\n",
    "\n",
    "Most machine learning libraries or packages will make some assumptions about the input and output format of the data.  Here we will also standardize on a format which is inspired by one such library.\n",
    "\n",
    "We will assume that our data is structured as a dataframe with a special column called \"label\" for the classification target.  So for a problem with two features, it might look like:\n",
    "\n",
    "|Feature 1|Feature 2|label|\n",
    "|---|---|---|\n",
    "|$x_{0,0}$|$x_{0,1}$|$y_{0}$|\n",
    "|$x_{1,0}$|$x_{1,1}$|$y_{1}$|\n",
    "|$\\vdots$|$\\vdots$|$\\vdots$|\n",
    "|$x_{n,0}$|$x_{n,1}$|$y_{n}$|\n",
    "\n",
    "Please make sure that your code is passing all the `assert` statements before you move onto the next part. Do not assume however, that the `assert` statements will catch all cases, you will need to do your own testing as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bd1516-f207-4b67-b7cb-d835c0c4f724",
   "metadata": {},
   "source": [
    "# Problem 1  - Train a decision Tree\n",
    "\n",
    "We start with building a decision tree classifier and implement both the training and inference.  As this is our first machine learning algorithm, we will take it slow and build it in many small steps and then put it all together.\n",
    "\n",
    "Our goal will be to build up a decision tree model using a variant of the C4.5 algorithm. This algorithm is an extension of the ID3 algorithm and solves many of the limitations of that algorithm. \n",
    "\n",
    "For more materials on this algorithm, please peruse\n",
    "\n",
    "- https://en.wikipedia.org/wiki/C4.5_algorithm\n",
    "\n",
    "\n",
    "The first few parts of this algorithm will be to build up a decision tree training algorithm, the second parts are to explore what to do with it.\n",
    "\n",
    "The algorithm proceeds as follows:\n",
    "\n",
    "1. Start with a dataset.  \n",
    "2. Check the base case for terminating recursion\n",
    "3. Choose the splitting criteria which maximizes gain (i.e. which feature for which you want to split)\n",
    "4. Split the dataset on this critera and recurse on each split\n",
    "\n",
    "The root node of the decision tree sees the entire training dataset, whereas the child nodes will only see that subset of the entire dataset that results from the splitting of the dataset at each parent level in the decision tree. \n",
    "\n",
    "For simplicity, we will consider the base case for the dataset at a given node,  which will lead to the termination of the recursion:\n",
    "\n",
    "1. All the examples in the dataset have the same label\n",
    "2. The number of examples in the dataset is less than or equal to MAX_EXAMPLES_PER_NODE, whose value can be set to some small fixed number (say 3). \n",
    "\n",
    "Lets start by building up some of the helper functions which we will need as part of the algorithm.\n",
    "\n",
    "## Part A\n",
    "\n",
    "First we will need to define a function to compute the entropy of a pandas series comprising the label column in the dataset. we define entropy as \n",
    "\n",
    "$$H(D) =  \\sum_i - p_i \\ln_2(p_i)$$\n",
    "\n",
    "where the summation index $i$ is over the unique labels in the label column, and $p_i$ is the proportion of the $i$'th label in the target column of the dataset.  For any data set the entropy can be calculated from the training data quite easily. Be careful with log evaluations in this expression, and note that $p_i \\\\ln_2(p_i) = 0$ for $p_i = 0$ (which acn be verified by evaluating this expression in the limit $p_1 \\rightarrow 0$  via L'Hospital's rule).\n",
    "\n",
    "For the case of a dataset, the probability of each element occuring can be calculated from the training data quite easily. \n",
    "\n",
    "For the sake of checking your initial answers please use the following dataset and define assertions for testing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ac478fc-f863-4ff6-8d98-d278d9b4b14d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>dog</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>cat</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>dog</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>lion</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a     b  label\n",
       "0  1   cat   True\n",
       "1  2   dog  False\n",
       "2  3   cat   True\n",
       "3  4   dog  False\n",
       "4  2  lion   True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "trial_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3, 4, 2],\n",
    "    'b': ['cat', 'dog', 'cat', 'dog', 'lion'],\n",
    "    'label': [True, False, True, False, True]\n",
    "})\n",
    "trial_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36bb4d7c-535c-4fc0-a5d5-142ae24fee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy so we can use the almost equal method\n",
    "import numpy as np\n",
    "def assert_almost_equal(a, b):\n",
    "    \"\"\"assert almost equal with readable error message\"\"\"\n",
    "    assert np.isclose(a, b), f\"{a} != {b}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "881fc82e-05e1-4c74-8325-f5bf98dd164a",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6aabfde25b8bed6a6051207d4c41707f",
     "grade": false,
     "grade_id": "cell-91a03164810e3c3e",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def freq(s:pd.Series) -> dict:\n",
    "    # Compute the frequency of each unique element\n",
    "    freq_dict = {}\n",
    "    for data in s: \n",
    "        freq_dict[data] = freq_dict.get(data, 0) + 1\n",
    "    return freq_dict\n",
    "\n",
    "def probability(s:pd.Series) -> dict:\n",
    "    # Compute the frequency of each unique element\n",
    "    freq_dict = freq(s)\n",
    "    # Calculate the probability distribution\n",
    "    for k in freq_dict:\n",
    "        freq_dict[k] = freq_dict[k]/len(s)\n",
    "    return freq_dict\n",
    "\n",
    "def entropy(s: pd.Series) -> float:\n",
    "    en = 0\n",
    "    prob = probability(s)\n",
    "    for p in prob.values():\n",
    "        en += -p*np.log2(p)\n",
    "    return en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb1b13dc-998a-4447-90c7-aa719088e239",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a143345b2440c03ad6623624a7edcac3",
     "grade": true,
     "grade_id": "cell-a78390042976f4be",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(entropy(trial_df['a']), 1.9219280948873623)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c72a87-0bd7-4beb-9eda-cbb6b23c19b5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## OLD Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eda45e-1cbe-46b3-aece-012e9a1c3785",
   "metadata": {},
   "source": [
    "### Part B.1\n",
    "Now we consider the gain, which you can think of as the information that you gain from making a decision for a particular variable.  Intuitively, we want to choose the variable to make a split which will give us the most information once we choose.  In order to do this, we look at the difference in entropy between the full problem and the subset of the problem which would occur if we take a particular value as a given. \n",
    "\n",
    "Not that the potential split conditions you evaluate for each feature will depend on whether $ftr$ is a numerical or categorical feature, as follows\n",
    "\n",
    "$$ G(D, ele) = H(D) - \\sum P(D|ele)H(D|ele)$$\n",
    "where $H$ is entropy $D$ is the dataset and $(D|ele)$ is the dataset given a chosen element. $H(D)$ is the entropy of the dataset given the label column.\n",
    "\n",
    "The nature of the feature (either numerical or categorical) can be obtained from the dtype of the feature in the dateset.\n",
    "\n",
    "The return value from this function will be the tuple with the first element as max_gain value over all the splits in the feature, and the second element being a List containing either the split value (for a numerical feature, or the set of values in the first set of the split for the categorical feature).   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61273416-2db7-4714-ab6b-de5f92a46c7a",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad803a31de7a766f66e4dc970ec5879b",
     "grade": false,
     "grade_id": "cell-331f2f6f582a3f83",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gain(df: pd.DataFrame, ele: str, label_key: str) -> float:\n",
    "    sum = 0\n",
    "    for data in df[ele].unique():\n",
    "        subset = df[ df[ele] == data ]\n",
    "        sum += len(subset)/len(df[ele]) * entropy(subset[label_key])\n",
    "    return entropy(df[label_key]) - sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d340dcef-dcca-4950-8fa2-797bb570bfa1",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35b1ef0b44da84ecd67f693310048ab2",
     "grade": true,
     "grade_id": "cell-7a003b2e429b3c47",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(gain(trial_df, 'b', 'label'), 0.9709505944546686)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dcf50b-965a-4e97-8b60-aa05ce257f6a",
   "metadata": {},
   "source": [
    "### Part B.2\n",
    "\n",
    "Now we can compute the gain ratio for a dataframe.  The gain ratio is defined as \n",
    "\n",
    "$$ GR = \\frac{G}{SI}$$\n",
    "\n",
    "Where $SI$ is the split information defined as \n",
    "$$ -\\sum_i \\frac{N_i}{|N|}\\log_2\\left(\\frac{N_i}{|N|}\\right)$$\n",
    "where $N_i$ is the number of occurencies of a unique value for a particular element and $|N|$ is the total number of elements in that dataset.\n",
    "\n",
    "In other words, I have a dataset like \n",
    "\n",
    "|Color of Shirt|\n",
    "|--------|\n",
    "| red|\n",
    "|blue|\n",
    "|red|\n",
    "|blue|\n",
    "|blue|\n",
    "\n",
    "Then the split info would be computed as \n",
    "\n",
    "$$ SI(\\text{Color of Shirt}) = -\\left[\\frac{N_{red}}{|N|}\\log_2\\left(\\frac{N_{red}}{|N|}\\right) + \\frac{N_{blue}}{|N|}\\log_2\\left(\\frac{N_{blue}}{|N|}\\right)\\right] = - \\left[\\frac{2}{5}\\log_2\\left(\\frac{2}{5}\\right) + \\frac{3}{5}\\log_2\\left(\\frac{3}{5}\\right)\\right]\\approx 0.971$$\n",
    "\n",
    "Please feel free to leverage the following resources:\n",
    "\n",
    "- https://machinewithdata.com/2018/07/10/how-to-calculate-gain-ratio/\n",
    "\n",
    "Please define a function `gain_ratio(df, ele, label_key)` which computes the gain ratio of column `ele` in dataframe `df` where the label column name is given as `label_key`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac1101f1-f7ca-48a0-be00-95cbc064e4c2",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a5f3ede4e442f57b8ba9535be03ec2a",
     "grade": false,
     "grade_id": "cell-6758c9a51eeb8506",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def gain_ratio(df: pd.DataFrame, ele: str, label_key: str) -> float:\n",
    "    freq_dict = freq(df[ele])\n",
    "    SI = 0\n",
    "    for data in freq_dict:\n",
    "        p = freq_dict[data]/len(df[ele])\n",
    "        SI += p*np.log2(p)\n",
    "    if SI == 0: SI = 1\n",
    "    return gain(df, ele, label_key)/(-SI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f389c929-2caa-452a-82f7-2426a2d823df",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e72f5b82d21f2e6a8fbea3f97988ecf",
     "grade": true,
     "grade_id": "cell-b96d042aea36da35",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert_almost_equal(gain_ratio(trial_df, 'b', 'label'), 0.6379740263133316)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2fa772-9d53-4862-8c3f-21615186ce0d",
   "metadata": {},
   "source": [
    "### Part B.3\n",
    "\n",
    "So far we have only handled categorical variables, we would like to also handle numerical variables.  What we will do is create a categorial variable out of our numeric variables by create a point $p$ and then each point is either $<=p$ or $>p$.  Once we create this point, we have a categorical feature of True, False values.  The trick here is how to compute the correct $p$.  The way we will do this is as follows:\n",
    "\n",
    "1.  What we do is for each value of the attribute, we split the dataset into all values less than or equal to that value and all values greater than that value.  \n",
    "2. We then compute the gain ratio as normal.  \n",
    "3. Take the $p$ which maximizes the gain ratio and use that in our fit.\n",
    "\n",
    "We now loop over all the features, calling `max_gain_for_feature`, and find the feature which leads to the maximum gain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25a867cb-b877-41ab-9417-b7151f346e23",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "61fc7aef3719f893140283a03f4c6d40",
     "grade": false,
     "grade_id": "cell-bd7c5f57ddb07b19",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def max_numeric_gain_ratio(df: pd.DataFrame, ele: str, label_key: str):\n",
    "    ele_uniq = df[ele].unique()\n",
    "    \n",
    "    # Initialize max gain ratio and corresponding threshold value\n",
    "    max_GR = 0\n",
    "    p = ele_uniq[0]\n",
    "    \n",
    "    # try different values of p\n",
    "    # compute the gain ration for each of the unique elements of df[ele] as p\n",
    "    # save the max gain ratio and corresponding p value\n",
    "    for data in ele_uniq:\n",
    "        df['over_p'] = df[ele] > data\n",
    "        GR_temp = gain_ratio(df, 'over_p', label_key)\n",
    "        if GR_temp > max_GR: \n",
    "            max_GR = GR_temp\n",
    "            p = data\n",
    "            \n",
    "    return (max_GR, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56d038ff-dc8a-4f46-9b3c-21425b7f0983",
   "metadata": {
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d4b85159ae34f35b114eb2ea71d096dd",
     "grade": true,
     "grade_id": "cell-28bc9424031b48a3",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = max_numeric_gain_ratio(trial_df, 'a','label')\n",
    "assert_almost_equal(res[0],  0.4459281986214854)\n",
    "assert res[1] == 3, f\"{res[1]} != 3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d673f8-0eaa-49f1-bb5d-8de50b529ec7",
   "metadata": {},
   "source": [
    "## NEW Part B\n",
    "\n",
    "Now we consider the information gain, which you can also equivalently think of as the reduction in entropy from making a binary split decision for a particular feature.  Intuitively, we want to choose the feature  which will give us the larget information gain (equivalently the largest entropy reduction) once we choose it to split the data set.  In order to do this, we first evaluate the information gain  between the full dataset and the two subsets obtained by  splitting the dataset using all posiblle conditions for a given feature column $ftr$.  We can then loop over all feature columns in this way to find the best choice of feature column (and the corresponding split condition using that column).\n",
    "\n",
    "Mathematically, if the feature $ftr$ is used to split the dataset $D$ (with $N$ examples) into two datasets $D_1$ and $D_2$ (with $N_1$ and $N_2$ examples respectively, then the Information Gain from this split  is denoted by\n",
    "\n",
    "$$ G(D, ftr) = H(D) - \\\\[\\\\frac{N_1){N}H(D_1) + \\\\frac{N_2){N}H(D_2) \\\\]$$. \n",
    "    \n",
    "Please define a function `max_gain_for_feature(df, ftr, label_key)` which computes the largest gain of spliting the dataset using the values in the column `ftr` in dataframe `df` where the label column name is given as `label`.\n",
    "\n",
    "Not that the potential split conditions you evaluate for each feature will depend on whether $ftr$ is a numerical or categorical feature, as follows\n",
    "\n",
    "- For numerical feature consider a set of potential split conditions as follows.  Le $k_1 < k_2, \\\\ldots k_J$ be the $J$ unique values in ascending order in the column and consider all splits of the form $(x \\\\leq k_i, x > k_i)$ for $i = 1, 2, \\\\ldots, J-1$.\n",
    "- For categorical features consider a set of potential split conditions as follows: Let (a, b, c, d) be the unique values of the feature column in some initial arbitrary order, and consider the 3 splits of the form [(a), (b,c,d)], [(a,b), (c,d)], [(a,b,c), (d)]  In general if there are k unique values in some initial arbitrary order then there will be (k-1) split conditions.\n",
    "\n",
    "The nature of the feature (either numerical or categorical) can be obtained from the dtype of the feature in the dateset.\n",
    "\n",
    "The return value from this function will be the tuple with the first element as max_gain value over all the splits in the feature, and the second element being a List containing either the split value (for a numerical feature, or the set of values in the first set of the split for the categorical feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d33cff-769c-471a-b979-61e15e777d96",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmax_gain_for_feature\u001b[39m(df: \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame, \n\u001b[1;32m      2\u001b[0m          ftr: \u001b[38;5;28mstr\u001b[39m, \n\u001b[1;32m      3\u001b[0m          label: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (\u001b[38;5;28mfloat\u001b[39m, List):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def max_gain_for_feature(df: pd.DataFrame, \n",
    "         ftr: str, \n",
    "         label: str) -> (float, List):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4e31b6-47ac-4fc7-9adf-7d119d5db0e9",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "We now loop over all the features, calling `max_gain_for_feature`, and find the feature which leads to the maximum gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ba592-2177-44ca-b9c3-cfcdaf508466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split condition(df: pd.DataFrame,  \n",
    "                              label: str) -> (str, List):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89204e7d-f546-4ccc-8512-ba3118aa9d77",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "Lets start by implementing the simple id3 algorithm which will give us a good sense of how to do the more complex algorithm.\n",
    "\n",
    "We will make a fake dataset of 10 observations with a label of true false for how well someone sleeps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed9b663d-651a-4477-b1c1-3790b5f5a202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day_type</th>\n",
       "      <th>weekend</th>\n",
       "      <th>good_night_before</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>long</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>short</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>medium</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>short</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>short</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>medium</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>long</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>short</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>short</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  day_type  weekend  good_night_before  label\n",
       "0     long     True               True   True\n",
       "1     long    False               True  False\n",
       "2    short    False              False  False\n",
       "3   medium    False               True  False\n",
       "4    short    False               True   True\n",
       "5    short    False              False  False\n",
       "6   medium     True              False   True\n",
       "7     long     True              False  False\n",
       "8    short     True              False   True\n",
       "9    short    False               True  False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id3_data = pd.DataFrame({\n",
    "    'day_type': ['long', 'long', 'short', 'medium', 'short', 'short', 'medium', 'long', 'short', 'short'],\n",
    "    'weekend': [True, False, False, False, False, False, True, True, True, False],\n",
    "    'good_night_before': [True, True, False, True, True, False, False, False, False, True],\n",
    "    'label': [True, False, False, False, True, False, True, False, True, False]\n",
    "})\n",
    "id3_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e35dec-8670-44de-81c5-008182a89e87",
   "metadata": {},
   "source": [
    "Next we will define an `Id3Node` class which will be used to hold the data for our algorithm.  Each of these nodes will be the node in a tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "01f296fd-989b-48b7-a8e2-07262da739c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "@dataclass\n",
    "class Id3Node:\n",
    "    key: str\n",
    "    children: Optional[Dict[str, 'Node']] = None\n",
    "    \n",
    "    def insert(self, kids: [str]):\n",
    "        # if children never initialized, initialize it to a dictionary\n",
    "        if self.children == None:\n",
    "            self.children = {}\n",
    "        # if just one child to insert\n",
    "        if type(kids) == str:\n",
    "            self.children[kids] = Id3Node(key=kids)\n",
    "        else:\n",
    "            # add the chidlren to the dictionary\n",
    "            for child in kids:\n",
    "                self.children[child] = Id3Node(key=child)\n",
    "\n",
    "    def __str__(self, node=\"start\", b=0, ans=''):\n",
    "        buffer = \"–––>\"\n",
    "        if node == \"start\": node = self\n",
    "        ans += node.key\n",
    "        # if the child has children, print each of the children\n",
    "        if node.children: #or len(self.children) > 0\n",
    "            for child_key in node.children:\n",
    "                # append buffer and child to answer string\n",
    "                ans += f\"\\n{' '*b}{buffer}{node.__str__(node.children[child_key], b+len(buffer))}\"\n",
    "        return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1ed492dd-74e7-4552-80b6-9b34cbb24147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "–––>child1\n",
      "    –––>child1a\n",
      "    –––>child1b\n",
      "–––>child2\n",
      "–––>child3\n"
     ]
    }
   ],
   "source": [
    "root = Id3Node('root')\n",
    "root.insert(['child1', 'child2', 'child3'])\n",
    "root.children['child1'].insert(['child1a', 'child1b'])\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af0c9ab-35ec-476a-8957-5141db28a5e5",
   "metadata": {},
   "source": [
    "Now we can implement the `train_id3` method.  This method should take a dataframe and return an `Id3Node` which represents the trained tree.  \n",
    "\n",
    "It is quite likely (although not strictly required) that your algorithm be recursive in nature.  Like any recursive algorithm, play close attention to the base cases defined above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3b7037c2-6836-45bd-ab3a-61bfce495d6d",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7717c5fadb91f054799f6d3bf816fb55",
     "grade": false,
     "grade_id": "cell-89579c32d1cdc0d7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def __str__(self, node=\"start\", b=0, ans=''):\n",
    "    if node == \"start\": node = self\n",
    "    ans += node.key\n",
    "    # if the child has children, print each of the children\n",
    "    if node.children: #or len(self.children) > 0\n",
    "        for child_key in node.children:\n",
    "            # append buffer and child to answer string\n",
    "            ans += node.__str__(node.children[child_key], b+len(buffer))\n",
    "    return ans\n",
    "\n",
    "def build_tree(df: pd.DataFrame, ele: str, root: Id3Node) -> Id3Node:  #, root: str\n",
    "    # Create a node for each unique value in the data\n",
    "    # Base case: we are at the label column. \n",
    "    return\n",
    "\n",
    "def train_id3(df: pd.DataFrame, label_key = 'label', root = Id3Node('root')) -> Id3Node:\n",
    "    print('cur:', root)\n",
    "    # Base case: df is empty\n",
    "    if len(df) == 0:\n",
    "        print('end')\n",
    "        return root\n",
    "    \n",
    "    # Base case: we only have the label column. \n",
    "    # if len(df) == 1 and df.columns[0] == label_key:\n",
    "        # print(len(df))\n",
    "        # print(df)\n",
    "        # for data in df[label_key].unique():\n",
    "        #     root.insert(f\"{label_key}_{data}\")\n",
    "        # return root\n",
    "        \n",
    "    max_gain = 0\n",
    "    best_ele = df.columns[0]\n",
    "    # for each col in the df except the last, compute the gain function and find the best gain    \n",
    "    for ele in df.loc[:, df.columns!=label_key]: \n",
    "        gain = gain_ratio(df, ele, df.columns[-1])\n",
    "        if gain > max_gain:\n",
    "            max_gain = gain\n",
    "            best_ele = ele\n",
    "\n",
    "    for data in df[best_ele].unique():\n",
    "        root.insert(f\"{best_ele}_{data}\")\n",
    "        # get a subset of the df where all the rows match data, and don't include the best_ele column\n",
    "        # train on this subset and add as child to this node\n",
    "        train_id3(df.loc[df[best_ele] == data, df.columns!=best_ele], label_key, root.children[f\"{best_ele}_{data}\"])\n",
    "    \n",
    "    # split the data based on the best column\n",
    "    # NOTE: need another function for best split value within the column\n",
    "    # build_tree(df, best_ele, root) #, df[best_ele].unique()[-1]\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "218f4dd2-0c08-4dbf-848c-4c782be84b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cur: root\n",
      "cur: weekend_True\n",
      "cur: day_type_long\n",
      "cur: good_night_before_True\n",
      "cur: label_True\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[123], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# for col in id3_data: print(col)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# first_col = id3_data.columns[0]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# print(id3_data[first_col].unique()[-1])\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Test base case train_id3\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m id3_tree \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_id3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mid3_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(id3_tree)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# display(id3_data.loc[id3_data['weekend'] == True, id3_data.columns!='weekend'])\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[122], line 44\u001b[0m, in \u001b[0;36mtrain_id3\u001b[0;34m(df, label_key, root)\u001b[0m\n\u001b[1;32m     41\u001b[0m     root\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_ele\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# get a subset of the df where all the rows match data, and don't include the best_ele column\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# train on this subset and add as child to this node\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtrain_id3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_ele\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43mbest_ele\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_ele\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# split the data based on the best column\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# NOTE: need another function for best split value within the column\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# build_tree(df, best_ele, root) #, df[best_ele].unique()[-1]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "Cell \u001b[0;32mIn[122], line 44\u001b[0m, in \u001b[0;36mtrain_id3\u001b[0;34m(df, label_key, root)\u001b[0m\n\u001b[1;32m     41\u001b[0m     root\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_ele\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# get a subset of the df where all the rows match data, and don't include the best_ele column\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# train on this subset and add as child to this node\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtrain_id3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_ele\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43mbest_ele\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_ele\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# split the data based on the best column\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# NOTE: need another function for best split value within the column\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# build_tree(df, best_ele, root) #, df[best_ele].unique()[-1]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "    \u001b[0;31m[... skipping similar frames: train_id3 at line 44 (1 times)]\u001b[0m\n",
      "Cell \u001b[0;32mIn[122], line 44\u001b[0m, in \u001b[0;36mtrain_id3\u001b[0;34m(df, label_key, root)\u001b[0m\n\u001b[1;32m     41\u001b[0m     root\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_ele\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# get a subset of the df where all the rows match data, and don't include the best_ele column\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;66;03m# train on this subset and add as child to this node\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mtrain_id3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbest_ele\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43mbest_ele\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mbest_ele\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdata\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# split the data based on the best column\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# NOTE: need another function for best split value within the column\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# build_tree(df, best_ele, root) #, df[best_ele].unique()[-1]\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root\n",
      "Cell \u001b[0;32mIn[122], line 32\u001b[0m, in \u001b[0;36mtrain_id3\u001b[0;34m(df, label_key, root)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Base case: we only have the label column. \u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# if len(df) == 1 and df.columns[0] == label_key:\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m# print(len(df))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m#     root.insert(f\"{label_key}_{data}\")\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# return root\u001b[39;00m\n\u001b[1;32m     31\u001b[0m max_gain \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 32\u001b[0m best_ele \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# for each col in the df except the last, compute the gain function and find the best gain    \u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ele \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mloc[:, df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m!=\u001b[39mlabel_key]: \n",
      "File \u001b[0;32m/opt/tljh/user/lib/python3.10/site-packages/pandas/core/indexes/base.py:5366\u001b[0m, in \u001b[0;36mIndex.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   5363\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mor\u001b[39;00m is_float(key):\n\u001b[1;32m   5364\u001b[0m     \u001b[38;5;66;03m# GH#44051 exclude bool, which would return a 2d ndarray\u001b[39;00m\n\u001b[1;32m   5365\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mcast_scalar_indexer(key)\n\u001b[0;32m-> 5366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgetitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5368\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[1;32m   5369\u001b[0m     \u001b[38;5;66;03m# This case is separated from the conditional above to avoid\u001b[39;00m\n\u001b[1;32m   5370\u001b[0m     \u001b[38;5;66;03m# pessimization com.is_bool_indexer and ndim checks.\u001b[39;00m\n\u001b[1;32m   5371\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_slice(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "# for col in id3_data: print(col)\n",
    "# first_col = id3_data.columns[0]\n",
    "# print(id3_data[first_col].unique()[-1])\n",
    "# for i in id3_data['weekend'].unique():\n",
    "#     print(i)\n",
    "\n",
    "# Test base case train_id3\n",
    "id3_tree = train_id3(id3_data)\n",
    "print(id3_tree)\n",
    "\n",
    "# display(id3_data.loc[id3_data['weekend'] == True, id3_data.columns!='weekend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed872f-a9c0-4b01-a079-8251e70845cc",
   "metadata": {},
   "source": [
    "Now we can create a `predict_id3` method which takes in a single row of a `DataFrame` (which is a `Series`) and a trained `Id3Node` (the root node) to make a prediction.\n",
    "\n",
    "The prediction should take the row and walk down the tree at each step choosing the proper child given the row information until it gets to a leaf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e50306-92a8-4dd2-8b79-625b10513853",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "102a316dfb5d8d689ed518e6ef3aec54",
     "grade": true,
     "grade_id": "cell-3b04a7162c97cc06",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_id3(row: pd.Series, node: Id3Node):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698a5f77-25d8-4530-b891-def9522c629f",
   "metadata": {},
   "source": [
    "Now put it all together, fill in the `fit` and `predict` methods for the `Id3Model` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d46ea4-65f3-40b1-97b0-3f9f4ed4ae57",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ce3278f4062a29dd4d5006e71d6fad5",
     "grade": true,
     "grade_id": "cell-ae496f77f77055bb",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ModelNotTrainedError(ValueError):\n",
    "    \"\"\"model is not trained yet\"\"\"\n",
    "        \n",
    "class Id3Model:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "    \n",
    "    def fit(self, df):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def predict(self, df):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1d083-26eb-43a1-8419-ddd0665e3963",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part E (BONUS)\n",
    "\n",
    "This is a bonus part to the problem.  It is not trivial to implement, but I encourage you to try!  Partial or correct solutions to this part will be worth some extra credit.\n",
    "\n",
    "We have already implemented the ID3 algorithm, now you can use the functions before to implement the same for the numeric types.\n",
    "\n",
    "Now we can put it all together, implement a training algorithm which produces a trained tree. To help you, we will define a class `Node` as well as a visualize function to produce an image of the node, you can choose to use your own data structure if you so wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b0d0f-db51-4be1-a706-9d264b5beb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "@dataclass\n",
    "class Node:\n",
    "    key: str\n",
    "    numeric: bool\n",
    "    pivot: float\n",
    "    children: Dict[str, 'Node']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2704a-59d6-4725-893e-8727793e9c24",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302d32116c9eee298aab3500c2fefb0a",
     "grade": true,
     "grade_id": "cell-287faaaf5ff85def",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f0deb7-c3c7-44c2-855a-17a07656b573",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Part F\n",
    "\n",
    "Now use this algorithm to fit the wine dataset below.  Give an overview of your results and an explanation of your findings.  If you did not do the bonus problem in part e), you may use the class below or the `scikit-learn` model directly.  The class is a very simple wrapper to conform to the dataframe format we have been using.\n",
    "\n",
    "\n",
    "Some questions to consider:\n",
    "\n",
    "1. Do your results make sense?\n",
    "\n",
    "**Note**: It may take some time to train your algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8068a9-c31b-4469-be34-16f2c2d7f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "class DecisionTreeModel(DecisionTreeClassifier):\n",
    "    \n",
    "    def fit(self, df):\n",
    "        if 'label' not in df:\n",
    "            raise ValueError(\"Label is not in df\")\n",
    "        X = df.drop('label', axis=1)\n",
    "        self.columns_ = X.columns.tolist()        \n",
    "        super().fit(X.values, df['label'].values)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        X = df[self.columns_].values if isinstance(df, pd.DataFrame) else df\n",
    "        return super().predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd0df07-d35b-45d7-bf85-0ef79e731ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "dataset = load_wine()\n",
    "df = pd.DataFrame(dataset['data'], columns=dataset['feature_names'])\n",
    "df['label'] = dataset['target']\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac06a4d-45ed-4a4f-a491-35175648997b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Problem 2\n",
    "\n",
    "In this problem, we will implement the perceptron algorithm as defined in the book, below is the skeleton class we will want to implement.\n",
    "\n",
    "Your alogorithm should take as a hyperparametr `max_iters` which is the number of times it will iterate through the training set entirely.\n",
    "\n",
    "Please include a `get_params` method which returns a `PerceptronParams` class with the current parameters of the model.  Its your choice how you store these, but this method must return this object type as it will be used to validate your results.\n",
    "\n",
    "For consistency with the results, please initialize all weights and biases to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a6915-41e7-4c9f-87da-587ec96b05a1",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2b98356b28249c3f721070e98b8f8eb6",
     "grade": true,
     "grade_id": "cell-7cd0ee9cbbaed97b",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "@dataclass\n",
    "class PerceptronParams:\n",
    "    weights: List[float]\n",
    "    bias: float\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, max_iters=20):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def get_params(self) -> PerceptronParams:\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    def fit(self, df, callback=None):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def predict(self, df):\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed7c36-5381-4846-aa9e-40fa53ffe172",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Now lets explore linear separability with the same Iris dataset that was used in Lecture 4.  Check that lecture to see how the data was preprocessed for a binary classification (setosa versus non-setosa) with just 2 features (petal width and petal length, and replicate that dataset in the next code block\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2a9b3-ac08-4e84-9f01-9841815f025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75daeb5-f47b-4bd7-8a46-511bd28b8028",
   "metadata": {},
   "source": [
    "Now train a perceptron and plot the best fit line on top of the scatter plot from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258e775-2ae4-4c2e-b484-0b5700d0e079",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf5ab3aac4e653f1bf6c26a93d62f741",
     "grade": true,
     "grade_id": "cell-1f907e71b21bb412",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e385e368-4a31-469c-b62a-bec480e9a1e8",
   "metadata": {},
   "source": [
    "Now lets examine how the line evolves as the model is trained.  Make a plot showing how the line changes per iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dca6e80-c703-4838-bca7-13453bc64fae",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3aa5098659adcc91764b13f1a607d47b",
     "grade": true,
     "grade_id": "cell-572d2d980ae09171",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5db16bf-034e-43d8-a69c-d498b2331522",
   "metadata": {},
   "source": [
    "## Part B (Bonus - extra credit)\n",
    "\n",
    "For extra credit, implement averaging in the perceptron model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d019054-acca-427a-9982-944ffd4b6193",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c434d5b6617e0fe9a80a3ca68dee707",
     "grade": true,
     "grade_id": "cell-a17fa9016ea8266e",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AveragePerceptron(Perceptron):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d6db3a-1983-4129-95b9-67f609c342d0",
   "metadata": {},
   "source": [
    "Now make the same plot as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6ebf4-7177-48d6-9b51-d5005a17dc7e",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0b187f193dc01f48bb08f566d080a72",
     "grade": true,
     "grade_id": "cell-8651ac46df11daed",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf7f79a-add7-4c05-9ff6-472f81beea59",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Analyze the stability of the weights as a function of iteration, If you did part b, please include it in your analysis.  Some points to consider\n",
    "\n",
    "- How stable were they over time?\n",
    "- How many iterations were appropriate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef004a9-4a54-4c04-afa1-d2dcdb371333",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0d99b6519acb800302e028122299d72",
     "grade": true,
     "grade_id": "cell-940423dbc76c5042",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87a264c-1230-4b5c-a227-eb8fe10db31d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Problem 3 - Conceptual\n",
    "\n",
    "For this problem, write each answer in the cell immediately following the question which will be marked.\n",
    "\n",
    "## Part A\n",
    "\n",
    "The algorithm we have used to train the decision tree is a greedy algorithm.  Explain why we use a greedy algorithm and what consequences this has on the resulting model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c55ec5a-5728-41e6-bde5-29f93e3de7be",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f26c516a036bcfb63f795625786b7ea0",
     "grade": true,
     "grade_id": "cell-caf5149447628dd7",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ee7513-985a-4dee-82bd-f447830ec4b4",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "In our perceptron algorithm, *IN YOUR OWN WORDS*, why is it important to shuffle the datasets?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae89bec1-a9ac-48ec-ab80-6fe5a808f081",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4839332aeab51e9b658b0455db4a232c",
     "grade": true,
     "grade_id": "cell-b11934c7dab18f43",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c909ceb1-cbc6-4cb1-b91e-a17077838edd",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "Is a decision tree guaranteed to find a globally optimal solution?  If not, what are the barriers to creating an algorithm to find the globally optimal solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482cb103-85e3-4b12-a3bf-2137123f9fb9",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a3414c4a781fa090b0eae371e88b17e",
     "grade": true,
     "grade_id": "cell-b6a56fee4b473efe",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eb4238-2c5f-4b41-82f6-806b857e72a1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Problem 4 - Data Problem\n",
    "\n",
    "Problem 4 is the real world simulation problem.  Here I will simply give you a dataset and a problem, your goal is to solve the problem and list all of your assumptions as well as your results.  This is meant to simulate many of the types of problems you may see in the future on an interview.\n",
    "\n",
    "You will be evaluated on how well you use the techniques we have learned so far in the course, you will not need to have the best model, you will be evaluated more on how you think and explain your solution.\n",
    "\n",
    "Here we are going to use the very common dataset, california housing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e27826-b925-4e31-ac37-122782a548d5",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "First of all do some exploratory data analysis and report your findings.  The following function will get the data for you, please take it from there!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045abd6b-1910-487d-81fa-f38ae3ac2876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "blob = fetch_california_housing()\n",
    "df = pd.DataFrame(blob['data'], columns=blob['feature_names'])\n",
    "df[blob['target_names'][0]] = blob['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09920a8f-df39-423e-8435-5b79f4fb6148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display(df)\n",
    "# df.info()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261f50a3-9293-4a89-a6d4-2fc842c8d6f4",
   "metadata": {},
   "source": [
    "## Part B\n",
    "\n",
    "Before modeling, its often good to start with a baseline model, something simple which will give us some indication if the model we are building is actually learning something.  Lets start by building a model which predicts that the median housing price of a block is the median of the entire dataset.  Please compute the mean squared error of this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c7590d-6768-4e88-8c13-6b722504bcdc",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f1be37974247a5fe64146ed9de374afe",
     "grade": true,
     "grade_id": "cell-6dbc46a56ce14db0",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924fe09d-2faa-4f9d-bbf0-da420350d018",
   "metadata": {},
   "source": [
    "## Part C\n",
    "\n",
    "In this problem we are going to use a linear regression to explore this dataset.  We will learn more about scikit-learn in the future, here you will be provided with an object to fit a dataframe to a linear model.  You are welcome to use the `scikit-learn` object directly if you so choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884f195-b0bf-42f3-9b4c-cc25bce85853",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "class LinearModel:\n",
    "    def __init__(self, fit_intercept: bool = True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.target_variable: Optional[str] = None\n",
    "        self.columns: Optional[List[str]] = None\n",
    "        self.model: Optional[LinearRegression] = None\n",
    "        \n",
    "    def _check_fit(self):\n",
    "        if self.model is None:\n",
    "            raise RuntimeError(\"model is not yet fit\")\n",
    "        \n",
    "    def fit(self, df: pd.DataFrame, target_variable: str) -> 'LinearModel':\n",
    "        \"\"\"fit a dataframe and return the coefficients and intercept\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pd.DataFrame\n",
    "            Input dataframe for fitting, should contain target variable\n",
    "        target_variable: str\n",
    "            Target variable for fitting, must be in the dataframe\n",
    "        \"\"\"\n",
    "        self.target_variable = target_variable\n",
    "        X = df.drop(target_variable, axis=1)\n",
    "        y = df[target_variable].values\n",
    "        self.columns = X.columns\n",
    "        self.model = LinearRegression(fit_intercept=self.fit_intercept)\n",
    "        self.model.fit(X.values, y)\n",
    "        return self\n",
    "    \n",
    "    @property\n",
    "    def coef(self):\n",
    "        self._check_fit()\n",
    "        return dict(zip(self.columns, self.model.coef_))\n",
    "    \n",
    "    @property\n",
    "    def intercept(self) -> float:\n",
    "        self._check_fit()\n",
    "        return self.model.intercept_\n",
    "\n",
    "    \n",
    "    def predict(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Make predictions using the fitted model\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        df: pd.DataFrame\n",
    "            Input dataframe for prediction\n",
    "        \"\"\"\n",
    "        self._check_fit()\n",
    "        X = df[self.columns].values\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2beefcc-702f-4cee-a45f-b508f5e8c774",
   "metadata": {},
   "source": [
    "Start off by fitting the linear regression directly to the dataset and then interpret your results.  Remove the Latitude and Longitude information for now (more on this in the next part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd542e69-620d-4539-ad1e-ab83e6717e53",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d50fca57e2df155eafe3b73d3777f7c5",
     "grade": true,
     "grade_id": "cell-71f2bc4b66decbf2",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d75c2aa-30bb-4505-8f71-dd2b851eb27a",
   "metadata": {},
   "source": [
    "## Part D\n",
    "\n",
    "Latitude and Longitude are different sorts of features than the rest of the features in this dataset, please explain why.\n",
    "\n",
    "**Hint**: Try making a plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d849ed1-5a75-4914-a4f1-67d560af173e",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5a3414c4a781fa090b0eae371e88b17e",
     "grade": true,
     "grade_id": "cell-b6a56fee4b473efe",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "source": [
    "ANSWER: They only make sense together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6a86a4-b8d5-4987-b3c1-198f36ca1073",
   "metadata": {},
   "source": [
    "## Part E (Bonus)\n",
    "\n",
    "Now we are going to use the latitude and longitude to generate some new features.  Often in an ML problem, bringing more is how we can best improve our predictive accuracy.\n",
    "\n",
    "Instead of using directly, lets create a feature which is minimum distance from a \"major\" city, defined as having greater than .5 million people.\n",
    "\n",
    "I have looked up the following wikipedia:\n",
    "\n",
    "| City | Latitude | Longitude |\n",
    "| --- | ---| ---|\n",
    "|Los Angeles| 34.03| -118.15|\n",
    "|San Diego|32.4254| -117.0945|\n",
    "|San Francisco|37.4639| -122.2459|\n",
    "|San Jose |37.2010| -121.5326|\n",
    "|Sacramento|38.3454| -121.2940|\n",
    "\n",
    "You are welcome to use any distance metric you like, however, one good one would be from the geopy library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3161a1c9-b677-4a55-928e-4ea778c5517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_longs = [\n",
    "    (34.04, -118.15),\n",
    "    (32.4254, -117.0945),\n",
    "    (37.4639, -122.2459),\n",
    "    (37.2010, -121.5326),\n",
    "    (38.3454, -121.2940)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1182509-5898-4ff2-85d0-385507c96084",
   "metadata": {
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3967dff95fd27da77af8378cedb7468a",
     "grade": true,
     "grade_id": "cell-bd9ec73649132f14",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10d7a32-820e-41da-92c6-17bac525e28f",
   "metadata": {},
   "source": [
    "## Part F\n",
    "\n",
    "Now please create the best linear model that you can, produce a explanation of the decision you made and why this is a \"good\" model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574f3a24-6e9f-4404-a830-5e7dcc00a26c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
